{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <small>FACIAL EMOTION</small><br><font color=\"blueviolet\">RECOGNITION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv as csv\n",
    "from numpy import array, mean, std, expand_dims, argmax\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from imutils import resize\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35887 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "0            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
       "1            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
       "2            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
       "3            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
       "4            6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...     Training\n",
       "...        ...                                                ...          ...\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
       "\n",
       "[35887 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = csv('dataset/fer2013.csv') # load pixel-based dataset\n",
    "print(df.info()) # display dataset info\n",
    "df # display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features, num_labels = 64, 7\n",
    "width, height = 48, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (28709, 48, 48, 1) y_train (28709, 7)\n",
      "x_test (3589, 48, 48, 1) y_test (3589, 7)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "for index, row in df.iterrows():\n",
    "    val = row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            x_train.append(array(val,'float32'))\n",
    "            y_train.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            x_test.append(array(val,'float32'))\n",
    "            y_test.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n",
    "        \n",
    "# wrap them all in a single array\n",
    "x_train = array(x_train, 'float32')\n",
    "y_train = array(y_train, 'float32')\n",
    "x_test = array(x_test, 'float32')\n",
    "y_test = array(y_test, 'float32')\n",
    "\n",
    "# label y == categorical data\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=num_labels)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=num_labels)\n",
    "\n",
    "# normalization\n",
    "x_train -= mean(x_train, axis=0)\n",
    "x_train /= std(x_train, axis=0)\n",
    "x_test -= mean(x_test, axis=0)\n",
    "x_test /= std(x_test, axis=0)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "print(\"x_train {} y_train {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test {} y_test {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Facial_Emotion_Recognition\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 18, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 5, 5, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 1,915,975\n",
      "Trainable params: 1,915,463\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # 1st convolution layer\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(x_train.shape[1:])),\n",
    "    Conv2D(64,kernel_size= (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Dropout(.5),\n",
    "\n",
    "    # 2nd convolution layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2, 2)),\n",
    "    Dropout(.5),\n",
    "\n",
    "    # 3rd convolution layer\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2), strides=(2, 2)),\n",
    "    \n",
    "    # fully connected neural networks\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(.2),\n",
    "    Dense(num_labels, activation='softmax')\n",
    "], name=\"Facial_Emotion_Recognition\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "# model.fit(x=x_train, y=y_train, epochs=200, batch_size=1, shuffle=True, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model weights\n",
    "# fer_json = model.to_json()\n",
    "# with open(\"model/fer.json\", \"w\") as json_file:\n",
    "#     json_file.write(fer_json)\n",
    "# model.save_weights(\"model/fer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the saved model\n",
    "model = model_from_json(open(\"model/fer.json\", \"r\").read())\n",
    "model.load_weights('model/fer.h5')\n",
    "\n",
    "def start_camera():\n",
    "    face_haar_cascade = cv2.CascadeClassifier(\"E:\\DATA SCIENCE\\Digital Image Processing\\cascade\\haarcascade_frontalface_default.xml\")\n",
    "    video_capture = cv2.VideoCapture(0) # turn on camera\n",
    "    \n",
    "    while True:\n",
    "        _ ,frame = video_capture.read() # start real-time recording\n",
    "        frame = cv2.flip(frame, 1) # let camera behaves like a mirror\n",
    "        frame = resize(frame, width=1000) # resize window using imutils.resize\n",
    "        \n",
    "        gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces_detected:\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (255, 0, 0), thickness=5)\n",
    "            \n",
    "            roi_gray = gray_img[y:y+w,x:x+h] # cropping region of interest (roi) i.e. face area from image\n",
    "            roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "            img_pixels = image.img_to_array(roi_gray)\n",
    "            img_pixels = expand_dims(img_pixels, axis = 0)\n",
    "            img_pixels /= 255\n",
    "\n",
    "            predictions = model.predict(img_pixels)\n",
    "            max_index = argmax(predictions[0]) # find max indexed array\n",
    "\n",
    "            emotions = ('ANGRY', 'DISGUST', 'FEAR', 'HAPPY', 'SAD', 'SURPRISE', 'NEUTRAL')\n",
    "            predicted_emotion = emotions[max_index]\n",
    "            cv2.putText(frame, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 5)\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Facial Emotion Analysis (Press 'Q' to Quit Window)\", frame)\n",
    "\n",
    "        # if the `Q` key was pressed, break from the loop (stop the recording)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    video_capture.release() # turn off camera\n",
    "    del(video_capture) # delete video_capture object\n",
    "    cv2.destroyAllWindows() # close all cv2 windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_camera()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
